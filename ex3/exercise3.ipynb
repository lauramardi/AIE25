{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.translate.bleu_score import sentence_bleu\n",
    "import Levenshtein\n",
    "import gensim.downloader as api"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Leemos ambos resúmenes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['people', 'are', 'using', 'ai', 'chatbots', 'to', 'fill', 'junk', 'websites', 'with', 'ai-generated', 'text', 'that', 'attracts', 'paying', 'advertisers,', 'according', 'to', 'a', 'new', 'report', 'from', 'the', 'media', 'research', 'organization', 'newsguard', 'that', 'was', 'shared', 'exclusively', 'with', 'mit', 'technology', 'review.', 'over', '140', 'major', 'brands', 'are', 'paying', 'for', 'ads', 'that', 'end', 'up', 'on', 'unreliable', 'ai-written', 'sites,', 'likely', 'without', 'their', 'knowledge.', 'ninety', 'percent', 'of', 'the', 'ads', 'from', 'major', 'brands', 'found', 'on', 'these', 'ai-generated', 'news', 'sites', 'were', 'served', 'by', 'google,', 'though', 'the', 'company’s', 'own', 'policies', 'prohibit', 'sites', 'from', 'placing', 'google-served', 'ads', 'on', 'pages', 'that', 'include', '“spammy', 'automatically', 'generated', 'content.”', 'the', 'practice', 'threatens', 'to', 'hasten', 'the', 'arrival', 'of', 'a', 'glitchy,', 'spammy', 'internet', 'that', 'is', 'overrun', 'by', 'ai-generated', 'content,', 'as', 'well', 'as', 'wasting', 'massive', 'amounts', 'of', 'ad', 'money.\\n']\n",
      "118\n"
     ]
    }
   ],
   "source": [
    "with open(\"summary-1-flan-ul2--article1.txt\", \"r\", encoding=\"utf-8\") as file:\n",
    "    reference_summary = file.read().lower().split(' ')\n",
    "print(reference_summary)\n",
    "print(len(reference_summary))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['a', 'new', 'report', 'finds', 'that', 'sites', 'run', 'with', 'ai-generated', 'content', 'serve', 'ads', 'from', 'major', 'brands,', 'which', 'mostly', 'come', 'from', 'google.', 'some', 'of', 'those', 'sites', 'contained', 'dangerous', 'misinformation.', 'and', 'this', 'is', 'just', 'getting', 'started.', 'more', 'could', 'be', 'on', 'the', 'way.', '\"the', 'opaque', 'nature', 'of', 'programmatic', 'advertising', 'has', 'inadvertently', 'turned', 'major', 'brands', 'into', 'unwitting', 'supporters.\"\\n']\n",
      "53\n"
     ]
    }
   ],
   "source": [
    "with open(\"summary-2-flan-ul2--article1.txt\", \"r\", encoding=\"utf-8\") as file:\n",
    "    candidate_summary = file.read().lower().split(' ')\n",
    "print(candidate_summary)\n",
    "print(len(candidate_summary))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6.752107625974243e-232\n"
     ]
    }
   ],
   "source": [
    "# BLEU for syntactic similarity\n",
    "bleu_score = sentence_bleu(reference_summary, candidate_summary)\n",
    "print(bleu_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "No parece ser muy útil el BLEU score ya que no encuentra conjuntos de palabras consecutivas que sean iguales, por lo que el score sale casi 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Levenshtein Distance: 107\n"
     ]
    }
   ],
   "source": [
    "# Levenshtein distance\n",
    "lev_dist = Levenshtein.distance(reference_summary, candidate_summary)\n",
    "print(f\"Levenshtein Distance: {lev_dist}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La distancia de levenshtein indica que habría que hacer 107 cambios para pasar de un resumen a otro, lo cual indica que no se parecen demasiado al ser resumenes de 118 y 53 palabras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\FA507\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n",
      "WMD Distance: 0.840365499455094\n"
     ]
    }
   ],
   "source": [
    "# Use pretrained word embeddings\n",
    "word_vectors = api.load(\"word2vec-google-news-300\")  # Large model (~1.5GB)\n",
    "\n",
    "# Compute WMD distance to measure distance based on embeddings\n",
    "distance = word_vectors.wmdistance(reference_summary, candidate_summary)\n",
    "print(f\"WMD Distance: {distance}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Una distancia de 0.84 indica un nivel relativamente alto de disparidad entre los documentos, incluso haciendo uso de word embeddings, por lo que se puede concluir que los documentos no son muy similares"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
